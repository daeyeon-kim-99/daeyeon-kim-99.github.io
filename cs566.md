---
layout: default
title: 2D Lower-Body Joint Prediction for Squat Analysis
---
<style>
  body {
    background-color: #f8fafc;
    color: #1f2933;
    font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
      sans-serif;
  }

  .page-content {
    max-width: 960px;
    margin: 0 auto;
    padding: 2.5rem 1.5rem 3.5rem;
  }

  a {
    color: #2563eb;
  }
  a:hover {
    color: #1d4ed8;
  }

  h1, h2, h3 {
    color: #0f172a;
    letter-spacing: 0.02em;
  }

  .hero {
    background: radial-gradient(circle at top left, #e0f2fe 0, #f9fafb 45%, #e5e7eb 100%);
    border-radius: 1.5rem;
    padding: 2.2rem 2rem 2.3rem;
    margin-bottom: 2rem;
    box-shadow: 0 18px 40px rgba(15, 23, 42, 0.12);
  }

  .hero-text h1 {
    font-size: 2.1rem;
    margin-bottom: 0.5rem;
  }

  .hero-text .subtitle {
    font-size: 0.98rem;
    color: #4b5563;
    max-width: 640px;
  }

  .hero-text .eyebrow {
    font-size: 0.78rem;
    text-transform: uppercase;
    letter-spacing: 0.16em;
    color: #6b7280;
    margin-bottom: 0.4rem;
  }

  .hero-tags {
    margin-top: 1.1rem;
    display: flex;
    flex-wrap: wrap;
    gap: 0.4rem;
  }

  .tag {
    display: inline-flex;
    align-items: center;
    padding: 0.18rem 0.6rem;
    border-radius: 999px;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.04em;
    text-transform: uppercase;
    border: 1px solid transparent;
  }

  .tag-ai {
    background-color: #eef2ff;
    color: #3730a3;
    border-color: #c7d2fe;
  }

  .tag-health {
    background-color: #ecfdf5;
    color: #166534;
    border-color: #bbf7d0;
  }

  .tag-ml {
    background-color: #eff6ff;
    color: #1d4ed8;
    border-color: #bfdbfe;
  }

  blockquote {
    border-left: 4px solid #93c5fd;
    margin: 1.2rem 0;
    padding: 0.5rem 1rem;
    background-color: #eff6ff;
    border-radius: 0 0.75rem 0.75rem 0;
    color: #1e3a8a;
  }

  .footer-note {
    margin-top: 3rem;
    padding-top: 1.2rem;
    border-top: 1px solid #e5e7eb;
    font-size: 0.85rem;
    color: #6b7280;
  }

  .footer-sub {
    display: block;
    margin-top: 0.15rem;
    font-size: 0.8rem;
  }

  .page-header {
  background-image: linear-gradient(135deg, #1e293b, #0f766e);
  box-shadow: 0 12px 30px rgba(15, 23, 42, 0.3);
  border-bottom: none;
}

.page-header .project-name,
.page-header .site-title {
  color: #e0f2fe;            
  font-weight: 800;
  letter-spacing: 0.04em;
}

.page-header .project-tagline,
.page-header .site-description {
  color: #bae6fd;        
  font-weight: 500;
}

.page-header h1 {
  color: #e0f2fe !important;  
  font-weight: 800;
  letter-spacing: 0.04em;
}

.page-header h2 {
  color: #bae6fd !important; 
  font-weight: 500;
}
</style>

<div class="hero">
  <div class="hero-text">
    <p class="eyebrow">CS 566 Final Project Â· Fall 2025</p>
    <h1>2D Lower-Body Joint Prediction for Squat Analysis</h1>
    <p class="subtitle">
      Heatmap-based 2D keypoint detection for hips, knees, and ankles â€“
      a first step toward safe, AI-assisted squat analysis and rehabilitation.
    </p>
    <div class="hero-tags">
      <span class="tag tag-ai">AI &amp; CV</span>
      <span class="tag tag-health">Health &amp; Rehab</span>
      <span class="tag tag-ml">Pose Estimation</span>
    </div>
  </div>
</div>

---

## ğŸ‘‹ Overview & Motivation

Many people exercise or perform rehabilitation at home without direct help from professional trainers.  
For complex movements like **squats**, it is hard to tell whether hip, knee, and ankle alignment is safe and correct â€“ and small mistakes can lead to long-term joint issues.

This project focuses on:

- ğŸ”¹ Estimating **2D lower-body joints** (hips, knees, ankles) from a single RGB image  
- ğŸ”¹ Specializing on **six joints only**, to focus on squat posture  
- ğŸ”¹ Building a foundation for **future 3D squat analysis and feedback**

Through this work, I realized that for AI systems dealing with the human body:

> **â€œAccuracy is not enough â€“ reliability and stability are essential.â€**

Even tiny joint errors can cause big differences in rehabilitation movements where angles matter.

---

## ğŸ¯ Problem Setting

**Input**

- A cropped RGB image of a person, resized to **256 Ã— 192**.

**Output**

- 2D locations of **six lower-body joints**:
  - Left / Right hip  
  - Left / Right knee  
  - Left / Right ankle  

**Formulation**

We treat this as a **heatmap prediction** task:

- For each joint \( j \), the model predicts a heatmap  
  \( \hat{H}_j \in \mathbb{R}^{64 \times 48} \)  
- Brighter regions = higher confidence  
- Final coordinates = argmax over each heatmap

This lower-body-only setup is intentional:  
our long-term goal is to use these 2D keypoints for **3D squat reconstruction** and angle analysis.

---

## ğŸ“š Dataset & Preprocessing

We build our training data from the **COCO keypoints** dataset.

### COCO Subsets

- **Full COCO (~250K images)**  
  General poses, lots of variety, but many partially visible legs.

- **Mini COCO (20K images)**  
  Small subset used for quick baselines and debugging.

- **Lower-Body Fully Visible (60K images)**  
  Curated subset where **all six lower-body joints** are visible and annotated.  
  â†’ Cleaner supervision, more relevant to leg-focused tasks.

### Preprocessing Pipeline  
(implemented in `COCOKeypointsLowerGTBbox`)

1. Load COCO annotation and **person bounding box**  
2. Crop to the person and resize to **256 Ã— 192**  
3. Normalize using **ImageNet mean / std**  
4. For each of the 6 joints:
   - Downsample GT coordinate by 4 â†’ heatmap size **64 Ã— 48**
   - Draw a **Gaussian** centered at that location  
   - Store **visibility weight** from COCO `v` flag

Outputs per sample:

- `image` â†’ `(3, 256, 192)`  
- `target` â†’ `(6, 64, 48)` heatmaps  
- `target_weight` â†’ visibility mask for 6 joints

---

## ğŸ§  Model Architectures

We compare three **heatmap-based** models.

### 1. ResNet18 Baseline

A standard top-down pose estimation style model:

- **Backbone:** ResNet18 â†’ features `(512, 8, 6)`  
- **Head:** 3 deconv layers â†’ `(256, 64, 48)`  
- **Output:** `1Ã—1` conv â†’ **6 heatmaps** `(6, 64, 48)`

Serves as our main reference architecture.

---

### 2. ResNet18-FPN (Multi-scale Features)

To better handle scale and context, we add an **FPN-style** fusion:

- Take intermediate features (e.g., c3, c4, c5) from ResNet18  
- Project and upsample them to a common size  
- Fuse them: `L1 + L2 + L3` â†’ multi-scale feature map  
- Upsample to `(256, 64, 48)` via deconv  
- Final `1Ã—1` conv â†’ 6 heatmaps

This helps the model reason about:

- Local edges (knees, ankles)  
- Global body structure (hipâ€“kneeâ€“ankle alignment)

---

### 3. Simple Dilated CNN (Lower-Body Only)

A lightweight CNN customized for legs:

- Initial `7Ã—7` conv â†’ `(64, 128, 96)`  
- Stack of **dilated residual blocks** (dilation 1, 2, 4, 1)  
- Keep spatial resolution around `(64, 64, 48)`  
- Final `3Ã—3` + `1Ã—1` conv â†’ **6 heatmaps**

Dilated convs give a **large receptive field**  
â†’ the model â€œseesâ€ the whole hipâ€“kneeâ€“ankle chain without heavy downsampling.

---

## ğŸ“‰ Training & Loss Function (Details)

### Heatmap Targets

For each joint \( j \):

1. Convert GT coordinate from image space \((256Ã—192)\) â†’ heatmap space \((64Ã—48)\)  
2. Draw a **2D Gaussian** centered at \((x_j, y_j)\):  
   \[
   H_j(x, y) = \exp\Big(-\frac{(x - x_j)^2 + (y - y_j)^2}{2\sigma^2}\Big)
   \]
3. Invisible or unannotated joints â†’ zero heatmap + weight 0

### JointsMSELoss with `target_weight`

Implemented as `JointsMSELoss(use_target_weight=True)`:

1. For each sample \( i \), joint \( j \):  

   - Flatten heatmaps  
   - Compute per-joint MSE:
     \[
     \ell_{ij} = \frac{1}{N_p} \sum_p (\hat{H}_{ij}(p) - H_{ij}(p))^2
     \]

2. Multiply by **visibility weight** \( w_{ij} \):  

   - visible â†’ \(w_{ij}=1\)  
   - invisible / uncertain â†’ \(w_{ij}=0\)

3. Average over 6 joints:
   \[
   L_i = \frac{1}{6} \sum_{j=1}^{6} w_{ij} \, \ell_{ij}
   \]

4. Average over batch:
   \[
   L = \frac{1}{B} \sum_{i=1}^{B} L_i
   \]

> âš ï¸ Note: we still divide by **6**, even if some joints are invisible â†’  
> images with many invisible joints tend to have **artificially low loss**.  
> (ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¼ ìŠ¬ë¼ì´ë“œì—ì„œë„ ì„¤ëª…í•  ìˆ˜ ìˆìŒ)

### Optimization

- **Optimizer:** Adam  
- **Learning rate:** `1e-3`  
- **Weight decay:** `1e-4`  
- **Epochs:**  
  - ResNet18: 30â€“50  
  - Simple CNN: up to 80â€“160 epochs on 60K subset  

We also overfit a **single batch** to confirm that loss can go near zero and the implementation is correct.

---

## ğŸ“ Evaluation Metric: PCK@0.05

We use **PCK (Percentage of Correct Keypoints)** with a strict threshold:

- A joint prediction is **correct** if  
  distance(pred, GT) < `0.05 Ã— image size`  
- PCK@0.05 = fraction of correct joints across all images & all 6 joints  
- This is intentionally harsh:  
  small pixel errors â†’ counted as incorrect

This matches our long-term goal:  
for **rehab and squat form**, small angular errors matter.

---

## ğŸ§ª Experiments & Results

> ìˆ«ìëŠ” ìŠ¬ë¼ì´ë“œ ê¸°ì¤€ **ëŒ€ëµì ** ì •ë¦¬ (ë²”ìœ„ë§Œ ìœ ì§€)

### ResNet18 Â· Mini COCO (20K)

- **30 epochs**
  - Loss â‰ˆ **0.0037**  
  - PCK@0.05 â‰ˆ **0.33**

- **50 epochs**
  - Loss â‰ˆ **0.0040â€“0.0041**  
  - PCK@0.05 â‰ˆ **0.0â€“0.17**

ğŸ‘‰ Longer training on small data â†’ possible overfitting & unstable PCK.

---

### ResNet18-FPN

- **Full COCO (~250K), 30 epochs**
  - Loss â‰ˆ **0.0039**  
  - PCK@0.05 â‰ˆ **0.17**

- **60K lower-body fully visible**
  - Loss â‰ˆ **0.0034â€“0.0036**  
  - PCK@0.05 â‰ˆ **0.33**

ğŸ‘‰ Smaller but **cleaner** lower-body subset beats the huge dataset on PCK.  
Data quality & visibility matter a lot for this task.

---

### Simple Dilated CNN

- **60K subset, 80â€“160 epochs**
  - Loss â‰ˆ **0.00337**  
  - PCK@0.05 â‰ˆ **0.17â€“0.50** (depending on sample/setting)

ğŸ‘‰ Even a lightweight, leg-specialized model can be competitive,  
sometimes outperforming heavier models on certain poses.

---

## ğŸ‘€ Qualitative Results

We visualize:

- Original image + predicted joints / skeleton  
- Heatmaps for each joint overlaid on the image

Observations:

- Compact squat poses â†’  
  **sharp heatmaps** near GT; joints well aligned.
- Extended or occluded legs â†’  
  wider, more diffuse heatmaps; ankles sometimes missed.
- Even when PCK@0.05 = 0,  
  the prediction can be *visually close* â†’ PCK@0.1 would be more forgiving.

From a user perspective, **stable and smooth** keypoints over time may be more important than a single very precise frame.

---

## ğŸ’¬ Limitations & Discussion

- **Strict metric**: PCK@0.05 is hard; small errors are heavily punished.  
- **Single-frame model**: No explicit temporal smoothing â†’ jitter in videos.  
- **General dataset**: COCO is not squat-specific; many poses are unrelated.

Still, we learned that:

- Lower-body-only training is feasible and meaningful.  
- Multi-scale features & dilated convs help capture the full leg chain.  
- **Reliability & stability** are as important as accuracy in health-related AI.

> For real users, an unstable or wrong model  
> can be more dangerous than having no model at all.

---

## ğŸš€ Future Work

Planned directions:

1. **Self-attention on ResNet18-FPN**  
   Better capture long-range joint relationships.

2. **Temporal modeling**  
   Use sequences and temporal convolutions / recurrent models  
   â†’ directly reduce jitter and enforce smooth motion.

3. **3D squat analysis**  
   Use our 2D lower-body keypoints as input to a 3D model,  
   estimate knee / hip angles, and give actual squat feedback.

4. **Squat-specific dataset**  
   Collect squat videos with precise lower-body labels  
   and ground-truth angle / safety annotations.

---
