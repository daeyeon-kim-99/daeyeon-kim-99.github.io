---
layout: default
title: 2D Lower-Body Joint Prediction
---

<style>
  body {
    background-color: #f8fafc;
    color: #1f2933;
    font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
      sans-serif;
  }

  .page-content {
    max-width: 960px;
    margin: 0 auto;
    padding: 0 1.5rem 3.5rem;
  }

  a {
    color: #2563eb;
  }
  a:hover {
    color: #1d4ed8;
  }

  h1, h2, h3 {
    color: #0f172a;
    letter-spacing: 0.02em;
  }

  .page-header {
    background-image: linear-gradient(135deg, #1e293b, #0f766e);
    box-shadow: 0 12px 30px rgba(15, 23, 42, 0.3);
    border-bottom: none;
    padding-bottom: 4.5rem;
  }

  .page-header .project-name,
  .page-header .site-title {
    color: #e0f2fe;
    font-weight: 800;
    letter-spacing: 0.04em;
  }

  .page-header .project-tagline,
  .page-header .site-description {
    color: #bae6fd;
    font-weight: 500;
  }

  .page-header h1 {
    color: #e0f2fe !important;
    font-weight: 800;
    letter-spacing: 0.04em;
  }

  .page-header h2 {
    color: #bae6fd !important;
    font-weight: 500;
  }

  .header-cta {
    margin-top: -3.2rem;
    margin-bottom: 2.2rem;
    text-align: center;
    position: relative;
    z-index: 2;
  }

  .header-btn {
    display: inline-flex;
    align-items: center;
    gap: 0.45rem;
    padding: 0.55rem 1.6rem;
    border-radius: 999px;
    border: 1px solid rgba(191, 219, 254, 0.95);
    background: rgba(15, 23, 42, 0.26);
    color: #e5e7eb;
    font-size: 0.88rem;
    font-weight: 500;
    text-decoration: none;
    backdrop-filter: blur(10px);
    transition: all 0.16s ease-out;
  }

  .header-btn:hover {
    border-color: #38bdf8;
    background: rgba(15, 23, 42, 0.4);
    color: #f9fafb;
    box-shadow: 0 10px 25px rgba(15, 23, 42, 0.4);
    transform: translateY(-1px);
  }

  .header-btn svg {
    width: 17px;
    height: 17px;
    fill: currentColor;
  }

  blockquote {
    border-left: 4px solid #93c5fd;
    margin: 1.2rem 0;
    padding: 0.5rem 1rem;
    background-color: #eff6ff;
    border-radius: 0 0.75rem 0.75rem 0;
    color: #1e3a8a;
  }

  .footer-note {
    margin-top: 3rem;
    padding-top: 1.2rem;
    border-top: 1px solid #e5e7eb;
    font-size: 0.85rem;
    color: #6b7280;
  }

  .footer-sub {
    display: block;
    margin-top: 0.15rem;
    font-size: 0.8rem;
  }

  .hero-image {
    margin-top: 1.0rem;
    margin-bottom: 2.2rem;
    text-align: center;
  }

  .hero-image img {
    max-width: 100%;
    height: auto;
    border-radius: 1.25rem;
    box-shadow: 0 18px 45px rgba(15, 23, 42, 0.35);
  }

  /* figure cards for loss / PCK curves */
  .figure-grid {
    display: flex;
    flex-wrap: wrap;
    gap: 1rem;
    margin: 1rem 0 1.8rem;
  }

  .figure-card {
    flex: 1 1 260px;
    background-color: #ffffff;
    border-radius: 1rem;
    padding: 0.75rem 0.9rem 1rem;
    box-shadow: 0 12px 28px rgba(15, 23, 42, 0.12);
    border: 1px solid #e5e7eb;
  }

  .figure-card img {
    width: 100%;
    height: auto;
    border-radius: 0.75rem;
    display: block;
  }

  .figure-caption {
    font-size: 0.8rem;
    margin-top: 0.4rem;
    color: #4b5563;
  }
</style>

<div class="header-cta">
  <a class="header-btn"
     href="https://github.com/daeyeon-kim-99/CS566_Project"
     target="_blank" rel="noopener">
    <svg viewBox="0 0 16 16" aria-hidden="true">
      <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 
      7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 
      1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 
      0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 
      2.2.82a7.65 7.65 0 0 1 2-.27c.68 0 1.36.09 2 .27 1.53-1.04 
      2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 
      1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 
      1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 
      8.01 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
    </svg>
    View on GitHub
  </a>
</div>

<div class="hero-image">
  <img src="/assets/img/squat_demo.png" alt="Squat pose prediction demo">
</div>

## üëã Overview & Motivation

Many people exercise or perform rehabilitation at home without direct help from professional trainers.  
For complex movements like **squats**, it is hard to tell whether hip, knee, and ankle alignment is safe and correct ‚Äì and small mistakes can lead to long-term joint issues.

This project focuses on:

- üîπ Estimating **2D lower-body joints** (hips, knees, ankles) from a single RGB image  
- üîπ Specializing on **six joints only**, to focus on squat posture  
- üîπ Building a foundation for **future 3D squat analysis and feedback**

Through this work, I realized that for AI systems dealing with the human body:

> **‚ÄúAccuracy is not enough ‚Äì reliability and stability are essential.‚Äù**

Even tiny joint errors can cause big differences in rehabilitation movements where angles matter.

---

## üéØ Problem Setting

**Input**

- A cropped RGB image of a person, resized to **256 √ó 192**.

**Output**

- 2D locations of **six lower-body joints**:
  - Left / Right hip  
  - Left / Right knee  
  - Left / Right ankle  

**Formulation**

We treat this as a **heatmap prediction** task:

- For each joint \( j \), the model predicts a heatmap  
  \( \hat{H}_j \in \mathbb{R}^{64 \times 48} \)  
- Brighter regions = higher confidence  
- Final coordinates = argmax over each heatmap

This lower-body-only setup is intentional:  
our long-term goal is to use these 2D keypoints for **3D squat reconstruction** and angle analysis.

---

## üìö Dataset & Preprocessing

We build our training data from the **COCO keypoints** dataset.

### COCO Subsets

- **Full COCO (~250K images)**  
  General poses, lots of variety, but many partially visible legs.

- **Mini COCO (20K images)**  
  Small subset used for quick baselines and debugging.

- **Lower-Body Fully Visible (60K images)**  
  Curated subset where **all six lower-body joints** are visible and annotated.  
  ‚Üí Cleaner supervision, more relevant to leg-focused tasks.

### Preprocessing Pipeline  
(implemented in `COCOKeypointsLowerGTBbox`)

1. Load COCO annotation and **person bounding box**  
2. Crop to the person and resize to **256 √ó 192**  
3. Normalize using **ImageNet mean / std**  
4. For each of the 6 joints:
   - Downsample GT coordinate by 4 ‚Üí heatmap size **64 √ó 48**
   - Draw a **Gaussian** centered at that location  
   - Store **visibility weight** from COCO `v` flag

Outputs per sample:

- `image` ‚Üí `(3, 256, 192)`  
- `target` ‚Üí `(6, 64, 48)` heatmaps  
- `target_weight` ‚Üí visibility mask for 6 joints

---

## üß† Model Architectures

We compare three **heatmap-based** models.

### 1. ResNet18 Baseline

A standard top-down pose estimation style model:

- **Backbone:** ResNet18 ‚Üí features `(512, 8, 6)`  
- **Head:** 3 deconv layers ‚Üí `(256, 64, 48)`  
- **Output:** `1√ó1` conv ‚Üí **6 heatmaps** `(6, 64, 48)`

Serves as our main reference architecture.

---

### 2. ResNet18-FPN (Multi-scale Features)

To better handle scale and context, we add an **FPN-style** fusion:

- Take intermediate features (e.g., c3, c4, c5) from ResNet18  
- Project and upsample them to a common size  
- Fuse them: `L1 + L2 + L3` ‚Üí multi-scale feature map  
- Upsample to `(256, 64, 48)` via deconv  
- Final `1√ó1` conv ‚Üí 6 heatmaps

This helps the model reason about:

- Local edges (knees, ankles)  
- Global body structure (hip‚Äìknee‚Äìankle alignment)

---

### 3. Simple Dilated CNN (Lower-Body Only)

A lightweight CNN customized for legs:

- Initial `7√ó7` conv ‚Üí `(64, 128, 96)`  
- Stack of **dilated residual blocks** (dilation 1, 2, 4, 1)  
- Keep spatial resolution around `(64, 64, 48)`  
- Final `3√ó3` + `1√ó1` conv ‚Üí **6 heatmaps**

Dilated convs give a **large receptive field**  
‚Üí the model ‚Äúsees‚Äù the whole hip‚Äìknee‚Äìankle chain without heavy downsampling.

---

## üìâ Training & Loss Function

### Heatmap Targets

For each joint \( j \):

1. Convert GT coordinate from image space \((256√ó192)\) ‚Üí heatmap space \((64√ó48)\)  
2. Draw a **2D Gaussian** centered at \((x_j, y_j)\):  
   \[
   H_j(x, y) = \exp\Big(-\frac{(x - x_j)^2 + (y - y_j)^2}{2\sigma^2}\Big)
   \]
3. Invisible or unannotated joints ‚Üí zero heatmap + weight 0

### JointsMSELoss with `target_weight`

Implemented as `JointsMSELoss(use_target_weight=True)`:

1. For each sample \( i \), joint \( j \):  

   - Flatten heatmaps  
   - Compute per-joint MSE:
     \[
     \ell_{ij} = \frac{1}{N_p} \sum_p (\hat{H}_{ij}(p) - H_{ij}(p))^2
     \]

2. Multiply by **visibility weight** \( w_{ij} \):  

   - visible ‚Üí \(w_{ij}=1\)  
   - invisible / uncertain ‚Üí \(w_{ij}=0\)

3. Average over 6 joints:
   \[
   L_i = \frac{1}{6} \sum_{j=1}^{6} w_{ij} \, \ell_{ij}
   \]

4. Average over batch:
   \[
   L = \frac{1}{B} \sum_{i=1}^{B} L_i
   \]

> ‚ö†Ô∏è We still divide by **6**, even if some joints are invisible.  
> Images with many invisible joints can therefore show artificially low loss.

### Optimization

- **Optimizer:** Adam  
- **Learning rate:** `1e-3`  
- **Weight decay:** `1e-4`  
- **Epochs:**  
  - ResNet18: 30‚Äì50  
  - Simple CNN: up to 80‚Äì160 epochs on the 60K subset  

We also overfit a **single batch** to confirm that the loss implementation is correct.

---

## üìè Evaluation Metric: PCK@0.05

We use **PCK (Percentage of Correct Keypoints)** with a strict threshold:

- A joint prediction is **correct** if  
  distance(pred, GT) < `0.05 √ó image size`  
- PCK@0.05 = fraction of correct joints across all images & all 6 joints  

This is intentionally harsh ‚Äì small pixel errors are counted as incorrect,  
which matches our long-term goal for **rehab and squat form** where small angle errors matter.

---

## üß™ Experiments & Results

### ResNet18 ¬∑ Mini COCO (20K)

Training ResNet18 on the 20K mini-COCO subset shows how a simple baseline behaves on limited data.

<div class="figure-grid">
  <div class="figure-card">
    <img src="/assets/images/loss_curve30(20K).png"
         alt="ResNet18 mini COCO 20K - loss curve up to 30 epochs">
    <div class="figure-caption">
      ResNet18 ¬∑ mini COCO (20K) ‚Äì training/validation loss up to 30 epochs.
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/loss_curve_e50(20K).png"
         alt="ResNet18 mini COCO 20K - loss curve up to 50 epochs">
    <div class="figure-caption">
      ResNet18 ¬∑ mini COCO (20K) ‚Äì loss curve when training longer (50 epochs).
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/pck_curves30(20K).png"
         alt="ResNet18 mini COCO 20K - PCK curves up to 30 epochs">
    <div class="figure-caption">
      ResNet18 ¬∑ mini COCO (20K) ‚Äì PCK@0.05 evolution up to 30 epochs.
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/pck_curves_e50(20K).png"
         alt="ResNet18 mini COCO 20K - PCK curves up to 50 epochs">
    <div class="figure-caption">
      ResNet18 ¬∑ mini COCO (20K) ‚Äì PCK@0.05 when training is extended to 50 epochs.
    </div>
  </div>
</div>

---

### ResNet18-FPN ¬∑ Full COCO vs Lower-Body 60K

Here we compare FPN-based training on **full COCO (250K)** vs the curated **lower-body 60K** subset.

<div class="figure-grid">
  <div class="figure-card">
    <img src="/assets/images/loss_curve_e30(250K).png"
         alt="ResNet18-FPN full COCO 250K - loss curve">
    <div class="figure-caption">
      ResNet18-FPN ¬∑ full COCO (250K) ‚Äì loss curve for 30-epoch training.
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/pck_curves_e30(250K).png"
         alt="ResNet18-FPN full COCO 250K - PCK curve">
    <div class="figure-caption">
      ResNet18-FPN ¬∑ full COCO (250K) ‚Äì PCK@0.05 over 30 epochs.
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/loss_curve_e30(60K).png"
         alt="ResNet18-FPN lower-body 60K - loss curve">
    <div class="figure-caption">
      ResNet18-FPN ¬∑ lower-body fully visible 60K ‚Äì loss curve for 30 epochs.
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/pck_curve_e30(60K).png"
         alt="ResNet18-FPN lower-body 60K - PCK curve">
    <div class="figure-caption">
      ResNet18-FPN ¬∑ lower-body fully visible 60K ‚Äì PCK@0.05 over 30 epochs.
    </div>
  </div>
</div>

---

### Simple Dilated CNN ¬∑ Lower-Body 60K (80 vs 160 epochs)

Finally, we train the lightweight dilated CNN on the 60K lower-body subset with longer schedules (80 and 160 epochs).

<div class="figure-grid">
  <div class="figure-card">
    <img src="/assets/images/loss_curve_e80.png"
         alt="Dilated CNN lower-body 60K - loss curve 80 epochs">
    <div class="figure-caption">
      Dilated CNN ¬∑ 60K ‚Äì loss curve for 80-epoch training.
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/loss_curve_e160.png"
         alt="Dilated CNN lower-body 60K - loss curve 160 epochs">
    <div class="figure-caption">
      Dilated CNN ¬∑ 60K ‚Äì loss curve for 160-epoch training.
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/pck_curve_e80.png"
         alt="Dilated CNN lower-body 60K - PCK curve 80 epochs">
    <div class="figure-caption">
      Dilated CNN ¬∑ 60K ‚Äì PCK@0.05 over 80 epochs.
    </div>
  </div>
  <div class="figure-card">
    <img src="/assets/images/pck_curve_e160.png"
         alt="Dilated CNN lower-body 60K - PCK curve 160 epochs">
    <div class="figure-caption">
      Dilated CNN ¬∑ 60K ‚Äì PCK@0.05 over 160 epochs.
    </div>
  </div>
</div>

---

## üëÄ Qualitative Results

We visualize:

- Original image + predicted joints / skeleton  
- Heatmaps for each joint overlaid on the image

Observations:

- Compact squat poses ‚Üí  
  **sharp heatmaps** near GT; joints well aligned.
- Extended or occluded legs ‚Üí  
  wider, more diffuse heatmaps; ankles sometimes missed.
- Even when PCK@0.05 = 0,  
  the prediction can be *visually close* ‚Üí PCK@0.1 would be more forgiving.

From a user perspective, **stable and smooth** keypoints over time may be more important than a single very precise frame.

---

## üí¨ Limitations & Discussion

- **Strict metric**: PCK@0.05 is hard; small errors are heavily punished.  
- **Single-frame model**: No explicit temporal smoothing ‚Üí jitter in videos.  
- **General dataset**: COCO is not squat-specific; many poses are unrelated.

Still, we learned that:

- Lower-body-only training is feasible and meaningful.  
- Multi-scale features & dilated convs help capture the full leg chain.  
- **Reliability & stability** are as important as accuracy in health-related AI.

> For real users, an unstable or wrong model  
> can be more dangerous than having no model at all.

---

## üöÄ Future Work

Planned directions:

1. **Self-attention on ResNet18-FPN**  
   Better capture long-range joint relationships.

2. **Temporal modeling**  
   Use sequences and temporal convolutions / recurrent models  
   ‚Üí directly reduce jitter and enforce smooth motion.

3. **3D squat analysis**  
   Use our 2D lower-body keypoints as input to a 3D model,  
   estimate knee / hip angles, and give actual squat feedback.

4. **Squat-specific dataset**  
   Collect squat videos with precise lower-body labels  
   and ground-truth angle / safety annotations.

---