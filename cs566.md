---
layout: default
title: CS566 Project
---

### Individual Contributions

* **Daeyeon Kim**: Implemented the training pipeline and dataset preprocessing for lower-body COCO subsets; designed and trained the ResNet18, ResNet18-FPN, and simple dilated CNN models; performed quantitative and qualitative analysis; and wrote the majority of this report.
* **HaeSeung Pyun**: Helped with dataset curation and debugging, conducted experiments with different training schedules and subsets, contributed to result interpretation and slide preparation, and reviewed and edited the report.

---

# 2D Lower-Body Joint Prediction for Squat Analysis

This project focuses on estimating accurate **2D lower-body joints** (hips, knees, ankles) from single RGB images. Our goal is to build stable, heatmap-based keypoint detectors that can later be used for **3D squat analysis** and AI-assisted feedback for exercise and rehabilitation.

---

## Overview & Motivation

Many people exercise or perform rehabilitation at home without direct help from professional trainers. For complex movements like squats, it is difficult to maintain a safe and accurate posture, and small mistakes in hip, knee, or ankle alignment can lead to joint stress or long-term injury.

In this project, we explore how **2D lower-body joint estimation** can support safer squats. Instead of building a full end-to-end 3D system immediately, we first focus on designing and comparing strong 2D heatmap-based models specialized for the **six lower-body joints**. This becomes the foundation for future 3D reconstruction and angle-based feedback.

A key lesson from our work is that in health-related applications, **reliability and stability** are just as important as raw accuracy. Even tiny errors in joint localization can have a big impact on rehabilitation exercises where joint angles must be controlled precisely.

---

## Problem Setting

**Input:** A single RGB image (cropped around a person) of size (256 \times 192).

**Output:** Six 2D joint locations for the lower body:

* Left hip, Right hip
* Left knee, Right knee
* Left ankle, Right ankle

We model this as a **heatmap prediction problem**, where each joint is represented by a 2D heatmap indicating the probability that the joint is at each pixel. We later convert each heatmap to coordinates by taking the argmax.

This setup is intentionally lower-body-only, because our final goal is to analyze **squat form**, where leg alignment and knee/hip angles are crucial.

---

## Dataset & Preprocessing

We build our training data from the **COCO keypoints dataset** using three subsets:

* **Full COCO (~250K images)**
  General human images with diverse poses and occlusions. Good for learning general body structure, but many images have partially visible legs.

* **Mini COCO (20K images)**
  A small subset used for fast baseline experiments and model comparison.

* **Lower-Body Fully Visible Subset (60K images)**
  A curated subset where all six lower-body joints are visible and have valid annotations. This provides clean supervision specifically for the legs.

**Preprocessing pipeline (implemented in `COCOKeypointsLowerGTBbox`):**

1. Read COCO annotations and person bounding boxes.
2. Crop the image to the person box and resize to (256 \times 192).
3. Normalize the image using ImageNet mean and std (for compatibility with pretrained ResNet).
4. For each of the six joints:

   * Downsample the ground-truth coordinate by a factor of 4 to match the heatmap size ((64 \times 48)).
   * Generate a **Gaussian heatmap** centered at the joint location with a fixed standard deviation (\sigma).
   * Store a visibility flag based on COCO’s `v` value (visible, occluded, or not labeled).

The output of the dataset is:

* `image`: normalized tensor ((3, 256, 192)),
* `target`: ground-truth heatmaps ((6, 64, 48)),
* `target_weight`: visibility weights for each joint.

---

## Model Architectures

We compare three heatmap-based architectures:

### 1. ResNet18 Baseline

* **Backbone:** ResNet18 up to the final convolutional block, producing features of size ((512, 8, 6)).
* **Head:** A 3-layer deconvolution head progressively upsamples to ((256, 64, 48)).
* **Output:** A final (1 \times 1) convolution maps ((256, 64, 48) \rightarrow (6, 64, 48)) heatmaps.

This is our reference model: a standard top-down 2D pose estimation architecture adapted to six lower-body joints.

### 2. ResNet18-FPN (Feature Pyramid Network)

To better handle scale variation and global body structure, we add **multi-scale feature fusion**:

* Extract feature maps from multiple ResNet stages (e.g., c3, c4, c5).
* Project each to a common channel size and upsample to a common spatial size ((256, 32, 24)).
* Sum them: (L_1 + L_2 + L_3) to form a fused feature map.
* Add a deconvolution layer to reach ((256, 64, 48)).
* Output 6 heatmaps via a (1 \times 1) conv.

This FPN-style design lets the network combine both local detail and higher-level context, which is helpful for leg joints that depend on the overall pose.

### 3. Simple Dilated CNN for Lower-Body

We also design a **lightweight, leg-focused CNN**:

* Initial (7 \times 7) conv to get ((64, 128, 96)).
* A sequence of **dilated residual blocks** with dilation rates 1, 2, 4, 1, keeping features around ((64, 64, 48)).
* A (3 \times 3) conv followed by a final (1 \times 1) conv to output six heatmaps.

Dilated convolutions enlarge the receptive field without downsampling, so the model can “see” the full hip-knee-ankle chain while staying relatively small.

---

## Training & Loss Function

### Heatmap Generation (Target)

For each joint:

1. Take the ground-truth joint position in image coordinates.
2. Downsample by a factor of 4 to get heatmap coordinates.
3. Place a **2D Gaussian** centered at that location:
   [
   H_j(x, y) = \exp\left(-\frac{(x - x_j)^2 + (y - y_j)^2}{2\sigma^2}\right)
   ]
4. Normalize (or clip) the heatmap so values are in ([0, 1]).

Invisible or unannotated joints are given a zero heatmap and a visibility weight of 0.

### JointsMSELoss with Visibility Weights

Our loss is implemented as `JointsMSELoss(use_target_weight=True)` and works as follows:

1. For each sample (i) and joint (j), compute the **per-joint MSE** between predicted and target heatmaps:
   [
   \ell_{ij} = \frac{1}{N_p} \sum_{p} (\hat{H}*{ij}(p) - H*{ij}(p))^2
   ]
   where (N_p = 64 \times 48) is the number of pixels.

2. Multiply by the **visibility weight** (w_{ij}) from `target_weight`:

   * If the joint is visible (COCO `v` > 0.5), (w_{ij} = 1).
   * If the joint is missing or not reliable, (w_{ij} = 0), so it does not contribute to the loss.

3. Average over the 6 joints:
   [
   L_i = \frac{1}{6} \sum_{j=1}^{6} w_{ij} , \ell_{ij}
   ]

4. Finally, average over the mini-batch:
   [
   L = \frac{1}{B} \sum_{i=1}^{B} L_i
   ]

Because we still divide by 6 even when some joints are invisible, images with missing joints tend to produce a lower loss (some terms are zero). This is important when interpreting loss curves.

### Optimizer and Hyperparameters

* **Optimizer:** Adam
* **Initial learning rate:** (1 \times 10^{-3})
* **Weight decay:** (1 \times 10^{-4})
* **Epochs:** 30–50 for ResNet18; similar or longer for other models (e.g., 80–160 epochs for the simple model).
* **Mini-batch training:** shuffle data each epoch, standard training loop with forward → loss → backward → optimizer step.

We also performed small experiments where we overfit a single batch to verify that the model and loss work correctly (loss decreases and fits that batch almost perfectly).

---

## Evaluation Metrics

We use **PCK@0.05 (Percentage of Correct Keypoints)** as our main metric:

* For each joint, compute the Euclidean distance between predicted and ground-truth coordinates.
* If the distance is less than (0.05 \times) image size (based on width/height), the joint is counted as correct.
* PCK@0.05 is the fraction of correctly predicted joints across all images and all 6 joints.

PCK@0.05 is a **strict** threshold: even small errors cause the joint to be counted as incorrect. This is aligned with our long-term goal of **rehabilitation**, where small angular errors can matter.

We also monitor the **MSE loss** on the validation set to see convergence behavior.

---

## Experiments & Results

Here we summarize several key experiments. The numbers are approximate and come from our final slides.

### ResNet18 on Mini COCO (20K)

* **30 epochs:**

  * Loss ≈ 0.0037
  * PCK@0.05 ≈ 0.33

* **50 epochs:**

  * Loss ≈ 0.0040–0.0041
  * PCK@0.05 ≈ 0.0–0.17 (depending on the sample/setting)

Observation: longer training on the small 20K subset can overfit or destabilize PCK, even if the loss remains low.

### ResNet18-FPN

* **Full COCO (~250K), 30 epochs:**

  * Loss ≈ 0.0039
  * PCK@0.05 ≈ 0.17

* **60K Lower-Body Fully Visible:**

  * Loss ≈ 0.0034–0.0036
  * PCK@0.05 ≈ 0.33

Observation: the **cleaner 60K lower-body subset** gives better PCK than the full dataset, even though it is smaller. This suggests that data quality and visibility can be more important than dataset size for our task.

### Simple Dilated CNN

* **60K subset, 80–160 epochs:**

  * Loss ≈ 0.00337
  * PCK@0.05 around 0.17–0.50 in different conditions.

Observation: the simple model, despite being lighter, can achieve competitive loss and sometimes higher PCK on specific examples, which is promising for lower-body-specialized models.

---

## Qualitative Results

We visualize both:

* **Raw images with predicted joints (markers/lines)**, and
* **Heatmaps** for each joint overlaid on the image.

Some observations:

* When the legs are compact (e.g., in a squat with smaller spatial distance between joints), heatmaps are sharp and centered near ground truth.
* For more extended poses or strong occlusions, heatmaps become broader or shift to neighboring regions, and ankles are sometimes missed.
* Even when PCK@0.05 is “wrong,” the predicted heatmap may still be close in a visual sense. A relaxed threshold like PCK@0.1 would capture this.

These qualitative results are important because, for real users, slightly off but stable keypoints may still be good enough to estimate joint angles over time.

---

## Limitations & Discussion

* **Strict metric:** PCK@0.05 is very strict, and small pixel errors can significantly lower the score.
* **Single-frame models:** We operate on individual frames without temporal smoothing, so we do not explicitly penalize jitter across time.
* **Dataset mismatch:** COCO is a general-purpose dataset, not specifically for squats. Many images do not resemble typical strength-training poses.

From a rehabilitation perspective, our models are not yet ready for deployment. But they show that:

* Lower-body-only training can produce reasonable leg localization.
* Multi-scale features (FPN) and dilated convolutions help capture the global structure of the hips–knees–ankles chain.
* Data quality (full visibility) matters a lot.

Most importantly, we realized that **reliability and accountability** are key when AI touches human health. A system that sometimes gives unstable or inaccurate feedback could be worse than no system at all. Our future work is therefore focused on stability and interpretability, not just higher scores.

---

## Future Work

Some directions we would like to explore:

1. **Self-Attention on Top of ResNet18-FPN**
   Add a self-attention block to better model long-range joint relationships and emphasize important spatial locations.

2. **Temporal Modeling and Smoothing**
   Extend the model to video sequences, using temporal convolutions or recurrent modules to explicitly reduce jitter and stabilize joint trajectories.

3. **Integration with 3D Pose Estimation**
   Use our lower-body 2D keypoints as input to a 3D pose reconstructor (e.g., a Temporal 3D model) and compute squat angles and more detailed feedback.

4. **Task-Specific Squat Dataset**
   Collect a dedicated dataset of squat videos with accurate lower-body annotations and possibly ground-truth angles, which would better match our final use case.

---
