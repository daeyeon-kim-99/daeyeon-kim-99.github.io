---
layout: default
title: 2D Lower-Body Joint Prediction
---
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<style>
  body {
    background-color: #f8fafc;
    color: #1f2933;
    font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
      sans-serif;
  }

  .page-content {
    max-width: 1200px;
    margin: 0 auto;
    padding: 0 1.5rem 3.5rem;
  }

  a {
    color: #2563eb;
  }
  a:hover {
    color: #1d4ed8;
  }

  h1, h2, h3 {
    color: #0f172a;
    letter-spacing: 0.02em;
  }

  .page-header {
    background-image: linear-gradient(135deg, #1e293b, #0f766e);
    box-shadow: 0 12px 30px rgba(15, 23, 42, 0.3);
    border-bottom: none;
    padding-bottom: 4.5rem;
  }

  .page-header .project-name,
  .page-header .site-title {
    color: #e0f2fe;
    font-weight: 800;
    letter-spacing: 0.04em;
  }

  .page-header .project-tagline,
  .page-header .site-description {
    color: #bae6fd;
    font-weight: 500;
  }

  .page-header h1 {
    color: #e0f2fe !important;
    font-weight: 800;
    letter-spacing: 0.04em;
  }

  .page-header h2 {
    color: #bae6fd !important;
    font-weight: 500;
  }

  .header-cta {
    margin-top: -3.2rem;
    margin-bottom: 2.2rem;
    text-align: center;
    position: relative;
    z-index: 2;
  }

  .header-btn {
    display: inline-flex;
    align-items: center;
    gap: 0.45rem;
    padding: 0.55rem 1.6rem;
    border-radius: 999px;
    border: 1px solid rgba(191, 219, 254, 0.95);
    background: rgba(15, 23, 42, 0.26);
    color: #e5e7eb;
    font-size: 0.88rem;
    font-weight: 500;
    text-decoration: none;
    backdrop-filter: blur(10px);
    transition: all 0.16s ease-out;
  }

  .header-btn:hover {
    border-color: #38bdf8;
    background: rgba(15, 23, 42, 0.4);
    color: #f9fafb;
    box-shadow: 0 10px 25px rgba(15, 23, 42, 0.4);
    transform: translateY(-1px);
  }

  .header-btn svg {
    width: 17px;
    height: 17px;
    fill: currentColor;
  }

  blockquote {
    border-left: 4px solid #93c5fd;
    margin: 1.2rem 0;
    padding: 0.5rem 1rem;
    background-color: #eff6ff;
    border-radius: 0 0.75rem 0.75rem 0;
    color: #1e3a8a;
  }

  .footer-note {
    margin-top: 3rem;
    padding-top: 1.2rem;
    border-top: 1px solid #e5e7eb;
    font-size: 0.85rem;
    color: #6b7280;
  }

  .footer-sub {
    display: block;
    margin-top: 0.15rem;
    font-size: 0.8rem;
  }

  .hero-image {
    margin-top: 1.0rem;
    margin-bottom: 2.2rem;
    text-align: center;
  }

  .hero-image img {
    max-width: 100%;
    height: auto;
    border-radius: 1.25rem;
    box-shadow: 0 18px 45px rgba(15, 23, 42, 0.35);
  }

  /* Figure layout: loss / PCK side-by-side */
  .figure-grid {
    display: flex;
    flex-direction: column;
    gap: 1.6rem;
    margin: 1rem 0 2.2rem;
  }

  .figure-row {
    display: flex;
    flex-wrap: wrap;
    gap: 1rem;
    justify-content: center;
  }

  .figure-card {
    flex: 1 1 260px;
    max-width: 420px;
    background-color: #ffffff;
    border-radius: 1rem;
    padding: 0.75rem 0.9rem 1rem;
    box-shadow: 0 12px 28px rgba(15, 23, 42, 0.12);
    border: 1px solid #e5e7eb;
  }

  .figure-card img {
    width: 100%;
    height: auto;
    border-radius: 0.75rem;
    display: block;
  }

  .figure-caption {
    font-size: 0.8rem;
    margin-top: 0.4rem;
    color: #4b5563;
  }

  .exp-title {
    font-size: 0.95rem;
    font-weight: 600;
    color: #111827;
    margin: 0.2rem 0 0.2rem;
  }

  .exp-note {
    font-size: 0.85rem;
    color: #4b5563;
    margin: -0.2rem 0 0.6rem;
  }

  /* For vertically stacked, larger qualitative sample images */
  .vertical-stack {
    display: flex;
    flex-direction: column;
    gap: 1.5rem;
    align-items: center;
    margin-top: 0.8rem;
    margin-bottom: 1.8rem;
  }

  .sample-card {
    flex: none;
    width: 650px;
    max-width: 95%;
    background-color: #ffffff;
    border-radius: 1rem;
    padding: 1rem 1.2rem 1.4rem;
    box-shadow: 0 12px 28px rgba(15, 23, 42, 0.12);
    border: 1px solid #e5e7eb;
  }

  .sample-card img {
    width: 100%;
    max-width: 600px;
    height: auto;
    border-radius: 0.75rem;
    display: block;
    margin: 0 auto;
  }
</style>

<div class="header-cta">
  <a class="header-btn"
     href="https://github.com/daeyeon-kim-99/CS566_Project"
     target="_blank" rel="noopener">
    <svg viewBox="0 0 16 16" aria-hidden="true">
      <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 
      7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 
      1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 
      0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 
      2.2.82a7.65 7.65 0 0 1 2-.27c.68 0 1.36.09 2 .27 1.53-1.04 
      2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 
      1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 
      1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 
      8.01 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
    </svg>
    View on GitHub
  </a>
</div>

<div class="hero-image">
  <img src="/assets/images/sample1(60K).png" alt="Squat pose prediction demo (60K subset sample 1)">
</div>

# Overview & Motivation

Many people exercise or perform rehabilitation at home without direct help from professional trainers.  
For complex movements like **squats**, it is hard to tell whether hip, knee, and ankle alignment is safe and correct ‚Äì and small mistakes can lead to long-term joint issues.

This project focuses on:

üîπ Estimating **2D lower-body joints** (hips, knees, ankles) from a single RGB image

---

# Problem Setting

**Input**

- A cropped RGB image of a person, resized to **256 √ó 192**.

**Output**

- 2D locations of **six lower-body joints**:
  - Left / Right hip  
  - Left / Right knee  
  - Left / Right ankle  

**Formulation**

We treat this as a **heatmap prediction** task:

- For each joint *j*, the model predicts a heatmap of size **64 √ó 48**.  
- Brighter regions mean higher confidence.  
- The final joint coordinates are obtained by taking the **argmax** position in each heatmap.

---

# Dataset & Preprocessing

We build our training data from the **COCO keypoints** dataset.

### COCO Subsets

- **Full COCO (~250K annotations)**  
  General poses, lots of variety, but many partially visible legs.

- **Mini COCO (20K annotations)**  
  Small subset used for quick baselines and debugging.

- **Lower-Body Fully Visible (60K annotations)**  
  Filtered subset where **all six lower-body joints** are visible and annotated.  
  ‚Üí Cleaner supervision, more relevant to leg-focused tasks.

### Preprocessing Pipeline  

1. Load COCO annotation and **person bounding box**  
2. Crop to the person and resize to **256 √ó 192**  
3. Normalize using **ImageNet mean / std**  
4. For each of the 6 joints:
   - Downsample GT coordinate by 4 ‚Üí heatmap size **64 √ó 48**
   - Draw a **Gaussian** centered at that location  
   - Store **visibility weight** from COCO `v` flag

Outputs per sample:

- `image` ‚Üí `(3, 256, 192)`  
- `target` ‚Üí `(6, 64, 48)` heatmaps  
- `target_weight` ‚Üí visibility mask for 6 joints

---

# Model Architectures

We compare three **heatmap-based** models.

### 1. ResNet18 Baseline

A standard top-down pose estimation style model:

- **Backbone:** ResNet18 ‚Üí features `(512, 8, 6)`  
- **Head:** 3 deconv layers ‚Üí `(256, 64, 48)`  
- **Output:** `1√ó1` conv ‚Üí **6 heatmaps** `(6, 64, 48)`

---

### 2. ResNet18-FPN (Multi-scale Features)

To better handle scale and context, we add an **FPN-style**:

- Take intermediate features from ResNet18  
- Project and upsample them to a common size  
- Fuse them: `L1 + L2 + L3` ‚Üí multi-scale feature map  
- Upsample to `(256, 64, 48)` via deconv  
- Final `1√ó1` conv ‚Üí 6 heatmaps

This helps the model learn more about:

- Local edges (knees, ankles)  
- Global body structure (hip‚Äìknee‚Äìankle alignment)

---

### 3. Simple Dilated CNN

A lightweight CNN customized for legs:

- Initial `7√ó7` conv ‚Üí `(64, 128, 96)`  
- Stack of **dilated residual blocks** (dilation 1, 2, 4, 1)  
- Keep spatial resolution around `(64, 64, 48)`  
- Final `3√ó3` + `1√ó1` conv ‚Üí **6 heatmaps**

Dilated convs give a **large receptive field**  
‚Üí the model ‚Äúsees‚Äù the whole hip‚Äìknee‚Äìankle chain without heavy downsampling.

---

# Training & Loss Function

### Heatmap Targets

For each joint $j$:

1. Convert GT coordinate from image space $(256 \times 192)$ ‚Üí heatmap space $(64 \times 48)$  
2. Draw a **2D Gaussian** centered at $(x_j, y_j)$:

   $$
   H_j(x, y) = \exp\left( -\frac{(x - x_j)^2 + (y - y_j)^2}{2\sigma^2} \right)
   $$

3. Invisible or unannotated joints ‚Üí zero heatmap + weight 0

### JointsMSELoss with `target_weight`

Implemented as `JointsMSELoss(use_target_weight=True)`:

1. For each sample $i$, joint $j$:

   - Flatten heatmaps  
   - Compute per-joint MSE:

     $$
     \ell_{ij} = \frac{1}{N_p} \sum_p \left( \hat{H}_{ij}(p) - H_{ij}(p) \right)^2
     $$

2. Multiply by **visibility weight** $w_{ij}$:

   - visible ‚Üí $w_{ij} = 1$  
   - invisible / uncertain ‚Üí $w_{ij} = 0$

3. Average over 6 joints:

   $$
   L_i = \frac{1}{6} \sum_{j=1}^{6} w_{ij}\,\ell_{ij}
   $$

4. Average over batch:

   $$
   L = \frac{1}{B} \sum_{i=1}^{B} L_i
   $$

### Optimization

- **Optimizer:** Adam  
- **Learning rate:** `1e-3`  
- **Weight decay:** `1e-4`  
- **Epochs:**  
  - ResNet18: 30‚Äì50 on the 20K, 250K dataset  
  - ResNet18-FPN: 30 on the 60K subset  
  - Simple CNN: up to 80‚Äì160 epochs on the 60K subset  

---

# Evaluation Metric: PCK@0.05

We use **PCK (Percentage of Correct Keypoints)** with a strict threshold:

- A joint prediction is **correct** if  
  distance(pred, GT) < `0.05 √ó image size`  
- PCK@0.05 = fraction of correct joints across all images & all 6 joints  

This is intentionally harsh ‚Äì small pixel errors are counted as incorrect,  
which matches our long-term goal for **rehab and squat form** where small angle errors matter.

---

# Experiments & Results

> ‚ö†Ô∏è **Important note on loss interpretation**  
> Because the loss function divides by **6 joints regardless of visibility**,  
> images that contain **invisible or occluded lower-body joints** contribute  
> artificially low loss values during validation.  
> 
> In the validation set, many samples include legs that are partially outside the image  
> or fully occluded by objects or other people. These joints receive a visibility weight of 0  
> and thus **do not contribute to the heatmap error**, but the final loss is still divided by 6.  
> As a result:
> 
> - Samples with missing joints ‚Üí **lower loss**  
> - More missing joints ‚Üí **even lower loss**  
> 
> This effect causes the **validation loss to appear lower than the training loss**,  
> even when the model is not truly performing better on validation images.  

### ResNet18 ¬∑ Mini COCO (20K) + Full COCO (250K)

<div class="figure-grid">
  <div class="exp-title">20K mini COCO ¬∑ 30 epochs</div>
  <div class="figure-row">
    <div class="figure-card">
      <img src="/assets/images/loss_curve30(20K).png"
           alt="ResNet18 mini COCO 20K - loss curve up to 30 epochs">
      <div class="figure-caption">
        Training / validation loss (30 epochs).
      </div>
    </div>
    <div class="figure-card">
      <img src="/assets/images/pck_curves30(20K).png"
           alt="ResNet18 mini COCO 20K - PCK curves up to 30 epochs">
      <div class="figure-caption">
        PCK@0.05 over 30 epochs on the mini COCO subset.
      </div>
    </div>
  </div>

  <div class="exp-title">20K mini COCO ¬∑ 50 epochs</div>
  <div class="figure-row">
    <div class="figure-card">
      <img src="/assets/images/loss_curve_e50(20K).png"
           alt="ResNet18 mini COCO 20K - loss curve up to 50 epochs">
      <div class="figure-caption">
        Training / validation loss (50 epochs).
      </div>
    </div>
    <div class="figure-card">
      <img src="/assets/images/pck_curves_e50(20K).png"
           alt="ResNet18 mini COCO 20K - PCK curves up to 50 epochs">
      <div class="figure-caption">
        PCK@0.05 when training is extended to 50 epochs.
      </div>
    </div>
  </div>

  <div class="exp-title">Full COCO (250K) ¬∑ 30 epochs</div>
  <div class="figure-row">
    <div class="figure-card">
      <img src="/assets/images/loss_curve_e30(250K).png"
           alt="ResNet18 full COCO 250K - loss curve">
      <div class="figure-caption">
        ResNet18 ¬∑ full COCO (250K) ‚Äì loss over 30 epochs.
      </div>
    </div>
    <div class="figure-card">
      <img src="/assets/images/pck_curves_e30(250K).png"
           alt="ResNet18 full COCO 250K - PCK curve">
      <div class="figure-caption">
        ResNet18 ¬∑ full COCO (250K) ‚Äì PCK@0.05 over 30 epochs.
      </div>
    </div>
  </div>
</div>

---

### ResNet18-FPN ¬∑ Lower-Body 60K

<div class="figure-grid">
  <div class="exp-title">Lower-body fully visible 60K ¬∑ 30 epochs</div>
  <div class="figure-row">
    <div class="figure-card">
      <img src="/assets/images/loss_curve_e30(60K).png"
           alt="ResNet18-FPN lower-body 60K - loss curve">
      <div class="figure-caption">
        ResNet18-FPN ¬∑ lower-body 60K ‚Äì loss curve (30 epochs).
      </div>
    </div>
    <div class="figure-card">
      <img src="/assets/images/pck_curve_e30(60K).png"
           alt="ResNet18-FPN lower-body 60K - PCK curve">
      <div class="figure-caption">
        ResNet18-FPN ¬∑ lower-body 60K ‚Äì PCK@0.05 over 30 epochs.
      </div>
    </div>
  </div>
</div>

---

### Simple Dilated CNN ¬∑ Lower-Body 60K (80 vs 160 epochs)

<div class="figure-grid">
  <div class="exp-title">Dilated CNN on 60K ¬∑ 80 epochs</div>
  <div class="figure-row">
    <div class="figure-card">
      <img src="/assets/images/loss_curve_e80.png"
           alt="Dilated CNN lower-body 60K - loss curve 80 epochs">
      <div class="figure-caption">
        Dilated CNN ¬∑ 60K ‚Äì loss curve (80 epochs).
      </div>
    </div>
    <div class="figure-card">
      <img src="/assets/images/pck_curve_e80.png"
           alt="Dilated CNN lower-body 60K - PCK curve 80 epochs">
      <div class="figure-caption">
        Dilated CNN ¬∑ 60K ‚Äì PCK@0.05 over 80 epochs.
      </div>
    </div>
  </div>

  <div class="exp-title">Dilated CNN on 60K ¬∑ 160 epochs</div>
  <div class="figure-row">
    <div class="figure-card">
      <img src="/assets/images/loss_curve_e160.png"
           alt="Dilated CNN lower-body 60K - loss curve 160 epochs">
      <div class="figure-caption">
        Dilated CNN ¬∑ 60K ‚Äì loss curve (160 epochs).
      </div>
    </div>
    <div class="figure-card">
      <img src="/assets/images/pck_curve_e160.png"
           alt="Dilated CNN lower-body 60K - PCK curve 160 epochs">
      <div class="figure-caption">
        Dilated CNN ¬∑ 60K ‚Äì PCK@0.05 over 160 epochs.
      </div>
    </div>
  </div>
</div>

---

# Qualitative Comparison: Sample 1 & Sample 2

Below we provide qualitative comparisons across all models using **sample1** and **sample2**.  
The results visually support the experimental findings described earlier in the report and match the narrative from our presentation script.

## 1Ô∏è‚É£ ResNet18 ¬∑ Mini COCO (20K)

### **Sample 1 (20K)**

<div class="vertical-stack">
  <div class="sample-card">
    <img src="/assets/images/sample1(20K)_e30.png" alt="Sample 1 ‚Äî 20K ¬∑ 30 epochs">
    <div class="figure-caption">Sample 1 ‚Äî 20K ¬∑ 30 epochs</div>
  </div>

  <div class="sample-card">
    <img src="/assets/images/sample1(20K)_e50.png" alt="Sample 1 ‚Äî 20K ¬∑ 50 epochs">
    <div class="figure-caption">Sample 1 ‚Äî 20K ¬∑ 50 epochs</div>
  </div>
</div>

**Interpretation ‚Äî Sample 1**  
- Although the joints are relatively close together, the predicted heatmaps are **not very sharp**, indicating limited confidence and spatial precision.  
- With only 20K training samples, the model learns the rough joint structure but struggles to produce clean, high-quality heatmaps.  
- Increasing the training duration to 50 epochs does **not lead to dramatic improvement**; the heatmaps remain similarly diffuse.  
- This plateau in performance suggests that the model is **capacity-limited by the small dataset**, reinforcing the need to scale up the training data.

---

### **Sample 2 (20K)**

<div class="vertical-stack">
  <div class="sample-card">
    <img src="/assets/images/sample2(20K)_e30.png" alt="Sample 2 ‚Äî 20K ¬∑ 30 epochs">
    <div class="figure-caption">Sample 2 ‚Äî 20K ¬∑ 30 epochs</div>
  </div>

  <div class="sample-card">
    <img src="/assets/images/sample2(20K)_e50.png" alt="Sample 2 ‚Äî 20K ¬∑ 50 epochs">
    <div class="figure-caption">Sample 2 ‚Äî 20K ¬∑ 50 epochs</div>
  </div>
</div>

**Interpretation ‚Äî Sample 2**  
- This pose is more challenging: joints are farther apart and require stronger spatial reasoning.  
- The 20K dataset is not sufficient for robust learning.  
- Confidence maps are weak and unstable, even after more epochs.  
- This directly aligns with our script: *‚ÄúWe realized the dataset was too small, so we expanded it.‚Äù*

---

## 2Ô∏è‚É£ ResNet18-FPN ¬∑ Full COCO (250K)

### **Sample 1 (250K)**

<div class="vertical-stack">
  <div class="sample-card">
    <img src="/assets/images/sample1(250K).png" alt="Sample 1 ‚Äî Full COCO 250K">
    <div class="figure-caption">Sample 1 ‚Äî Full COCO 250K</div>
  </div>
</div>

**Interpretation ‚Äî Sample 1**  
- Larger dataset improves stability and overall accuracy.  
- FPN provides multi-scale context, helping the model better understand lower-body structure.  
- Heatmaps appear sharper and more consistent than in the 20K version.

---

### **Sample 2 (250K)**

<div class="vertical-stack">
  <div class="sample-card">
    <img src="/assets/images/sample2(250K).png" alt="Sample 2 ‚Äî Full COCO 250K">
    <div class="figure-caption">Sample 2 ‚Äî Full COCO 250K</div>
  </div>
</div>

**Interpretation ‚Äî Sample 2**  
- Performance is improved, but still limited by COCO's noisy and incomplete annotations.  
- As mentioned in the script:  
  *‚ÄúSome COCO images contain only 3 or fewer lower-body joints.‚Äù*  
- Therefore, model performance saturates despite dataset size.

---

## 3Ô∏è‚É£ ResNet18-FPN ¬∑ Lower-Body Fully Visible (60K)

### **Sample 1 (60K)**

<div class="vertical-stack">
  <div class="sample-card">
    <img src="/assets/images/sample1(60K).png" alt="Sample 1 ‚Äî 60K fully visible joints">
    <div class="figure-caption">Sample 1 ‚Äî 60K fully visible joints</div>
  </div>
</div>

**Interpretation ‚Äî Sample 1**  
- All six joints are guaranteed to be visible.  
- Supervision quality is much higher than in COCO.  
- Heatmaps show sharp, clear peaks and very stable predictions.  
- This supports our conclusion: filtered datasets yield better lower-body performance.

---

### **Sample 2 (60K)**

<div class="vertical-stack">
  <div class="sample-card">
    <img src="/assets/images/sample2(60K).png" alt="Sample 2 ‚Äî 60K fully visible joints">
    <div class="figure-caption">Sample 2 ‚Äî 60K fully visible joints</div>
  </div>
</div>

**Interpretation ‚Äî Sample 2**  
- Even on difficult poses, predictions are strong and confident.  
- Clear improvement over both 20K and 250K.  
- Shows that dataset *quality* is more important than dataset *size* for this task.

---

## 4Ô∏è‚É£ Simple Dilated CNN (80 / 160 epochs)

### **Sample 1 (Dilated CNN)**

<div class="vertical-stack">
  <div class="sample-card">
    <img src="/assets/images/sample1(e80).png" alt="Sample 1 ‚Äî Dilated CNN ¬∑ 80 epochs">
    <div class="figure-caption">Sample 1 ‚Äî Dilated CNN ¬∑ 80 epochs</div>
  </div>
</div>

**Interpretation ‚Äî Sample 1**  
- Works surprisingly well for compact joint configurations.  
- Dilated convolutions provide a sufficiently large receptive field.  
- This matches our script: *‚ÄúFor Sample 1, the simple model performs almost as well as the larger models.‚Äù*

---

### **Sample 2 (Dilated CNN)**

<div class="vertical-stack">
  <div class="sample-card">
    <img src="/assets/images/sample2(e80).png" alt="Sample 2 ‚Äî Dilated CNN ¬∑ 80 epochs">
    <div class="figure-caption">Sample 2 ‚Äî Dilated CNN ¬∑ 80 epochs</div>
  </div>
</div>

**Interpretation ‚Äî Sample 2**  
- Confidence is low and heatmaps are diffuse.  
- Simple model cannot capture long-range joint relationships.  
- This directly supports our next-step decision:  
  **‚ÄúAdd self-attention to better learn joint dependencies.‚Äù**

---

### Summary Across All Samples

- **20K dataset:** Too small ‚Üí unstable predictions  
- **250K COCO:** Large but noisy ‚Üí inaccurate lower-body learning  
- **60K fully-visible:** Best quality ‚Üí best predictions  
- **Dilated CNN:** Works only for simple poses ‚Üí struggles on complex ones  
- **Conclusion:**  
  ‚Üí Next model should be *larger* and include **self-attention** to capture joint relationships effectively.

---

## Limitations & Discussion

- **Strict metric**: PCK@0.05 is hard; small errors are heavily punished.  
- **Single-frame model**: No explicit temporal smoothing ‚Üí jitter in videos.  
- **General dataset**: COCO is not squat-specific; many poses are unrelated.

Still, we learned that:

- Lower-body-only training is feasible and meaningful.  
- Multi-scale features & dilated convs help capture the full leg chain.  
- **Reliability & stability** are as important as accuracy in health-related AI.

> For real users, an unstable or wrong model  
> can be more dangerous than having no model at all.

---

## Future Work

Planned directions:

1. **Self-attention on ResNet18-FPN**  
   Better capture long-range joint relationships.

2. **Temporal modeling**  
   Use sequences and temporal convolutions / recurrent models  
   ‚Üí directly reduce jitter and enforce smooth motion.

3. **3D squat analysis**  
   Use our 2D lower-body keypoints as input to a 3D model,  
   estimate knee / hip angles, and give actual squat feedback.

4. **Squat-specific dataset**  
   Collect squat videos with precise lower-body labels  
   and ground-truth angle / safety annotations.
